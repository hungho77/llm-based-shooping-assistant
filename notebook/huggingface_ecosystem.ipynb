{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%wandb disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and pre-process data\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq\n",
    "\n",
    "raw_datasets = load_dataset(\"SetFit/tweet_sentiment_extraction\")\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "task_description = \"Classify the following sentence: \"\n",
    "\n",
    "def tokenize_function(example):\n",
    "    prompt = task_description + example['text'] + f\" Label: {example['label_text']}{tokenizer.eos_token}\"\n",
    "    inputs = tokenizer(prompt, truncation=True)\n",
    "    inputs['labels'] = inputs['input_ids'][:]\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, remove_columns=['text', 'label', 'textID', 'label_text'])\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# 2. Train\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy='no', do_eval=False, do_train=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].select(range(100)),\n",
    "    eval_dataset=tokenized_datasets[\"test\"].select(range(10)),\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Hugging Face Hub\n",
    "\n",
    "As outlined earlier, transfer learning is one of the key factors driving the success of transformers because it makes it possible to reuse pretrained models for new tasks. Consequently, it is crucial to be able to load pretrained models quickly and run experiments with them.\n",
    "\n",
    "The Hugging Face Hub hosts over 500,000 freely available models and datasets. As weâ€™ve seen with the pipelines, loading a promising model in your code or loading a dataset is then just one line of code away. This makes experimenting with a wide range of models and datasets simple, and allows you to focus on the domain-specific parts of your project.\n",
    "\n",
    "![](https://i.imgur.com/SSTNZU3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤— Tokenizers\n",
    "\n",
    "Behind each of the pipeline examples that weâ€™ve seen is a tokenization step that *splits the raw text into smaller pieces* called **tokens**. It would be good to understand that tokens may be *words, parts of words, or just characters* like punctuation. Transformer models are trained on numerical representations of these tokens, so getting this step right is very important for the whole NLP project!\n",
    "\n",
    "ðŸ¤— Tokenizers provides many tokenization strategies and is extremely fast at tokenizing text thanks to its Rust backend. It also takes care of all the pre- and post-processing steps, such as normalizing the inputs and transforming the model outputs to the required format. With ðŸ¤— Tokenizers, we can load a tokenizer in the same way we can load pretrained model weights with   Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **tokenization** process is done by the tokenize() method of the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens.\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tokenizer is a **subword** tokenizer: it splits the words until it obtains tokens that can be represented by its vocabulary. Thatâ€™s the case here with `TinyaLlama`, which is split into five tokens: `_T`, `iny`, `L`, `l` and `ama`.\n",
    "\n",
    "The conversion to input IDs is handled by the `convert_tokens_to_ids()` tokenizer method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These outputs, once converted to the appropriate framework tensor, can then be used as inputs to a model as seen later in this notebook.\n",
    "\n",
    "**Decoding** is going the other way around: from vocabulary indices, we want to get a string. This can be done with the `decode()` method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_string = tokenizer.decode([450, 323, 4901, 29931, 29880, 3304, 2060, 263, 9893, 304, 758, 14968, 263, 29871, 29896, 29889, 29896, 29933, 365, 29880, 3304, 1904, 373, 29871, 29941, 534, 453, 291, 18897, 29889])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, and this is how we can utilize tokenizers to feed models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens.\",\n",
    "    \"The training has started on 2023-09-01.\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An increasingly common use case for LLMs is chat. In a chat context, rather than continuing a single string of text (as is the case with a standard language model), the model instead continues a conversation that consists of one or more messages, each of which includes a role, like â€œuserâ€ or â€œassistantâ€, as well as message text. Chat templates are part of the tokenizer. They specify how to convert conversations, represented as lists of messages, into a single tokenizable string in the format that the model expects. Refer to [Chat Template](https://huggingface.co/docs/transformers/main/en/chat_templating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}\n",
    "{% for message in messages %}\n",
    "{% if message['role'] == 'user' %}{{ bos_token + 'Classify the following sentence: ' + message['content'] + '\\n' }}{% elif message['role'] == 'assistant' %}{{ 'Label: '  + message['content'] + eos_token }}\n",
    "{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Label: ' }}{% endif %}\"\"\"\n",
    "\n",
    "tokenizer.chat_template = template\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n",
    "    # {\"role\": \"assistant\", \"content\": \"The sun.\"}\n",
    "]\n",
    "tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so here is how we would train an LLM as a sequence classifier on one batch in PyTorch.\n",
    "\n",
    "I recommend that you apply the chat template as a preprocessing step for your dataset. After this, you can simply continue like any other language model training task. When training, you should usually set `add_generation_prompt=False`, because the added tokens to prompt an assistant response will not be helpful during training. Letâ€™s see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "\n",
    "# Same as before\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "chat1 = [\n",
    "    {\"role\": \"user\", \"content\": \"what interview! leave me alone\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"negative\"}\n",
    "]\n",
    "chat2 = [\n",
    "    {\"role\": \"user\", \"content\": \"I really really like the song Love Story by Taylor Swift\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"positive\"}\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\n",
    "dataset = dataset.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})\n",
    "print(dataset['formatted_chat'][0])\n",
    "\n",
    "# Tokenize dataset\n",
    "batch = tokenizer(dataset['formatted_chat'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# Clone inputs for labels\n",
    "batch[\"labels\"] = batch['input_ids'].clone()\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
